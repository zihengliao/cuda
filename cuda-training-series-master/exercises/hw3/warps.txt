

when executing an instruction on gpu, it will automatically allocate instructions
in warps. Each warp contains 32 threads. 

Lets say you ask the gpu to use 48 threads, it will assign the instruction to
2 warps where the first warp all threads will be used, the 2nd warp, only the first 16 threads
are active, but the second half won't do anything despite it being used.

There is almost no to negligble performance loss when asking gpu to schedule more
warps/threads as opposed to a smaller amount of warps/threads.

Performance is measured in clock cycles in GPU. Everything you do has latency which
is measured in clock cycles. For example, to perform an arithmetic calculation,
this will take in orders of <10 clock cycles.

Read instructions are executed immediately, but may take a couple clock cycles to return a value.

Your goal is to "hide" latency by having an instruction execute every clock cycle.
This means running multiple warps concurrently and having them executing instructions
every clock cycle to make sure the GPU is busy.

## Question to self, why can't you have all the warps executing an instruction on the
    same clock cycle and then having them wait again until it's executed to perfrom the next
    instruction?

    - Because we only have limited ALUs and LD/ST units. We want to maximise throughput
    by keeping the GPU busy constantly 

You also have to be very aware of the GPU architecture since different GPUs have different
amount of units

For my 3060Ti, I have:
- 38SM
- 1x SM can process:
    - 48 warps
    - 1536 threads = 48 warps x 32 threads
    - 32 blocks

1x block can have:
    - 1024 threads
    - 32 warps = 1024 threads / 32 threads



Tests:

Test1 
    - 38SM x 32 blocks = 1216 blocks
    - 1024 threads
    - Duration                         us       976.45
    - Memory Throughput                 %        94.85

Test2 
    - 38SM x 32 blocks = 1216 blocks
    - 512 threads
    - Duration                         us       974.02
    - Memory Throughput                 %        95.05

Test3 
    - 38SM x 32 blocks = 1216 blocks
    - 256 threads
    - Duration                         us       982.02
    - Memory Throughput                 %        94.30

Test4 - testing excessive blocks:
    - 100000 blocks
    - 512 threads
    - Duration                         ms         1.03
    - Memory Throughput                 %        89.75

Test 5:
    - 1216 blocks
    - 1536 threads
    - does not work since threads exceed max possible amount per block

Test 6:
    - 1216 blocks
    - 64 threads
    - Duration                         us       988.54
    - Memory Throughput                 %        93.75


This is a common suggestion I get from CMD:
-     INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.